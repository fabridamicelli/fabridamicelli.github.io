{"cells": [{"cell_type": "raw", "id": "a0645709", "metadata": {}, "source": ["---\n", "author: Fabrizio Damicelli\n", "badges: false\n", "branch: master\n", "categories:\n", "- Python\n", "- uv\n", "- MLOps\n", "- Kubeflow\n", "- Pipelines\n", "comments: false\n", "date: '2025-08-25'\n", "description: A simple workflow to manage dependencies of Python Kubeflow components\n", "filters:\n", "- social-share\n", "output-file: kubeflow-uv.html\n", "share:\n", "  description: '\"Dependency Management with uv in Kubeflow Pipelines. A simple workflow\n", "    to manage dependencies of Python Kubeflow components\"'\n", "  linkedin: true\n", "  mastodon: true\n", "  permalink: '\"https://fabridamicelli.github.io/posts/kubeflow-uv.html\"'\n", "  reddit: true\n", "  twitter: true\n", "title: Dependency Management with uv in Kubeflow Pipelines\n", "toc: false\n", "\n", "---\n"]}, {"cell_type": "markdown", "id": "2ae8cf6c-c7f7-4424-931d-2d31f0bfaf56", "metadata": {}, "source": ["::: {.callout-note title=\"TL;DR\"}\n", "\n", "\n", "When defining a Python-based Kubeflow Pipeline component, you typically list dependencies using the `packages_to_install` argument in `kfp.dsl.component` or by baking them into a custom Docker image.\n", "\n", "I present here a `custom_component` wrapper that leverages [`uv`](https://docs.astral.sh/uv/) to automate this process by inferring dependencies from [dependency-group](https://packaging.python.org/en/latest/specifications/dependency-groups/) in the `pyproject.toml` file.\n", "\n", "See [code repo here](https://github.com/fabridamicelli/kubeflow-pipeline-uv)\n", ":::"]}, {"cell_type": "markdown", "id": "76c1e21e-10e5-433b-99da-5c51092ef5e0", "metadata": {}, "source": ["I presented this content as a talk at PyData Berlin 2025 and you can watch the recording here:\n", "\n", "{{< video https://vimeo.com/1115528220/7073eb4ffb >}}"]}, {"cell_type": "markdown", "id": "f133fb77-72a1-4e4b-a025-e35db686152b", "metadata": {}, "source": ["# Introduction\n", "\n", "[Kubeflow](https://www.kubeflow.org/) is an open-source project including a number of sub-projects like [Kubeflow Pipelines](https://www.kubeflow.org/docs/components/pipelines/) that I will talk about here. \n", "\n", "> Kubeflow is a platform for building and deploying portable and scalable machine learning (ML) workflows using containers on Kubernetes-based systems.\n", ">  A pipeline is a definition of a workflow that composes one or more components together to form a computational directed acyclic graph (DAG).\n", "\n", "In other words, Kubeflow is an orchestration tool for data/machine learning workflows.  \n", "This is the example pipeline we'll be working on:\n", "\n", "![](../images/kubeflow-uv-diagram.png){width=200 fig-align=\"center\"}"]}, {"cell_type": "markdown", "id": "830a0712-cb70-4e70-9db1-abd97e84771f", "metadata": {}, "source": ["Simple enough, a pipeline with 3 components:\n", "\n", "1) `download-dataset`:Fetch and prepare data\n", "2) `train`: Train a machine learning model\n", "3) `plot-confusion-matrix`: Plot model performance"]}, {"cell_type": "markdown", "id": "1a6e7ab9-02d3-4947-8ea4-e6fa8c484bf8", "metadata": {}, "source": ["Kubeflow offers several alternatives to define pipeline components and the easiest way is to use a so-called \"Python-based\" component that look like this:"]}, {"cell_type": "code", "execution_count": 15, "id": "5a955b03-2854-4fd4-b385-526d624bba2f", "metadata": {}, "outputs": [], "source": ["from kfp import dsl\n", "\n", "base_image = \"python:3.12\"\n", "\n", "@dsl.component(\n", "    base_image=base_image,\n", "    packages_to_install=[\"scikit-learn>=1.7.1\"]  # <--- Specify dependencies\n", ")\n", "def download_dataset(dataset: dsl.Output[dsl.Dataset]):\n", "    ...\n", "    \n", "@dsl.component(\n", "    base_image=base_image,\n", "    packages_to_install=[\"scikit-learn>=1.7.1\"]\n", ")\n", "def train(\n", "    dataset: dsl.Input[dsl.Dataset],\n", "    predictions: dsl.Output[dsl.Artifact]\n", "):\n", "    ...\n", "    \n", "@dsl.component(\n", "    base_image=base_image,\n", "    packages_to_install=[\n", "        \"matplotlib>=3.10.5\",\n", "        \"scikit-learn>=1.7.1\",\n", "        \"seaborn>=0.13.2\",\n", ")\n", "def plot_confusion_matrix(predictions: dsl.Input[dsl.Artifact]):\n", "    ...\n", "\n", "@dsl.pipeline\n", "def pipeline():\n", "    dataset_task = download_dataset()\n", "    train_task = train(dataset=dataset_task.outputs[\"dataset\"])\n", "    plot_task = plot_confusion_matrix(predictions=train_task.outputs[\"predictions\"])"]}, {"cell_type": "markdown", "id": "5377d18e-ca7d-4915-a1cc-43ed15c580d8", "metadata": {}, "source": ["That is the skeleton of the pipeline.\n", "If you are interested in the actual implementation of the components you can expand the code section below or check out [this repo](https://github.com/fabridamicelli/kubeflow-pipeline-uv) (but for now is not really necessary to understand the problem)."]}, {"cell_type": "code", "execution_count": null, "id": "2a3a51ce-6710-438b-97e5-644255367fc0", "metadata": {}, "outputs": [], "source": ["#| output: false\n", "#| code-fold: true\n", "\n", "@dsl.component\n", "def download_dataset(dataset: dsl.Output[dsl.Dataset]):\n", "    import pickle\n", "\n", "    from sklearn.datasets import fetch_20newsgroups\n", "    from sklearn.feature_extraction.text import TfidfVectorizer\n", "\n", "    def fetch_data(subset: str):\n", "        categories = [\"alt.atheism\", \"talk.religion.misc\", \"comp.graphics\", \"sci.space\"]\n", "        return fetch_20newsgroups(\n", "            subset=subset,\n", "            categories=categories,\n", "            shuffle=True,\n", "            random_state=42,\n", "        )\n", "\n", "    data_train, data_test = fetch_data(\"train\"), fetch_data(\"test\")\n", "    y_train, y_test = data_train.target, data_test.target\n", "    vectorizer = TfidfVectorizer(\n", "        sublinear_tf=True, max_df=0.5, min_df=5, stop_words=\"english\"\n", "    )\n", "    X_train = vectorizer.fit_transform(data_train.data)\n", "    X_test = vectorizer.transform(data_test.data)\n", "\n", "    out = {\n", "        \"X_train\": X_train,\n", "        \"y_train\": y_train,\n", "        \"X_test\": X_test,\n", "        \"y_test\": y_test,\n", "        \"target_names\": data_train.target_names,\n", "    }\n", "    with open(dataset.path, mode=\"wb\") as file:\n", "        pickle.dump(out, file)\n", "\n", "\n", "@component\n", "def train(\n", "    dataset: dsl.Input[dsl.Dataset],\n", "    predictions: dsl.Output[dsl.Artifact],\n", "):\n", "\n", "    import pickle\n", "    from sklearn.linear_model import RidgeClassifier\n", "\n", "    with open(dataset.path, mode=\"rb\") as file:\n", "        data = pickle.load(file)\n", "\n", "    clf = RidgeClassifier(tol=1e-2, solver=\"sparse_cg\")\n", "    clf.fit(data[\"X_train\"], data[\"y_train\"])\n", "    pred = clf.predict(data[\"X_test\"])\n", "\n", "    out = {\n", "        \"y_test\": data[\"y_test\"],\n", "        \"y_pred\": pred,\n", "        \"target_names\": data[\"target_names\"],\n", "    }\n", "    with open(predictions.path, mode=\"wb\") as file:\n", "        pickle.dump(out, file)\n", "\n", "@dsl.component\n", "def plot_confusion_matrix(\n", "    predictions: dsl.Input[dsl.Artifact],\n", "    confusion_plot: dsl.OutputPath(str),\n", "):\n", "    import pickle\n", "\n", "    import matplotlib.pyplot as plt\n", "    from sklearn.metrics import ConfusionMatrixDisplay\n", "    import seaborn as sns\n", "\n", "    sns.set_theme(style=\"white\", font_scale=1.2)\n", "\n", "    with open(predictions.path, mode=\"rb\") as file:\n", "        preds = pickle.load(file)\n", "\n", "    fig, ax = plt.subplots(figsize=(10, 5))\n", "    ConfusionMatrixDisplay.from_predictions(preds[\"y_test\"], preds[\"y_pred\"], ax=ax)\n", "    ax.xaxis.set_ticklabels(preds[\"target_names\"])\n", "    ax.yaxis.set_ticklabels(preds[\"target_names\"])\n", "    ax.set_title(\"Confusion Matrix\")\n", "    fig.savefig(confusion_plot)\n", "    plt.close()"]}, {"cell_type": "markdown", "id": "7f930aad-2809-4b63-8b0d-905e12d3166e", "metadata": {}, "source": ["# Executing the pipeline\n", "\n", "So far we have our pipeline expressed as a DAG (directed acyclic graph) with all the relationships between components.  \n", "Think of each component as a box that can take inputs, does computation and can produce output, where inputs/outputs mean any kind of artifact (data, machine learning model, etc.).\n", "\n", "![](../images/pipeline-components.png)"]}, {"cell_type": "markdown", "id": "bca7662e-2ee2-4227-9b37-b47f323ab4c5", "metadata": {}, "source": ["Here's the point where the \"magic\" from the `kpf` library comes in:\n", "`kpf` will prepare docker images specifications and each `component` will run in a separate, isolated docker container on whatever execution engine we choose (local laptop, cloud machine, etc.) and we can pass data between components by defining inputs/outputs.\n", "You can think of it as the body of the function being a script that runs inside the container.  \n", "One last thing, kubeflow will \"compile\" the pipeline definition into a serializable format (e.g., json, yaml), so that we can use that recipe to let the compute engine run our job or to log and/or reproduce the experiments.\n", "Here's what that looks like:\n", "\n", "```python\n", "from kfp import compiler\n", "\n", "compiler.Compiler().compile(pipeline, package_path='pipeline.yaml')\n", "```"]}, {"cell_type": "markdown", "id": "045c46d5-eca4-47b6-81ed-b4bf01effe6e", "metadata": {}, "source": ["# Component dependencies\n", "\n", "Each container image is built from a base image (for example, `python:3.12`) on top of which we install the particular dependencies, such as `scikit-learn`, `pandas`, etc., necessary to compute the step.\n", "And each component has specific python dependencies that in kubeflow get passed as the list of strings `packages_to_install` (see example above).\n", "\n", "***Here's where things get tricky.***\n", "\n", "Managing the dependencies of all the pipeline components is challenging \n", "because even if we pin the dependencies of the necessary libraries (e.g., \"pandas==2.3.1\"), transitive dependencies (libraries that those libraries depend on) can break things in unpredictable ways (ever heard of the numpy 2.0 release? \n", "\ud83d\ude43).  \n", "Beyond that, adjusting the dependencies of each component by hand definitely does not scale and we want to be able to profit from tooling such as [dependabot](https://github.com/dependabot), which does not understand the list \"packages_to_install\", for security fixes that allow us to adjust the whole dependency graph as needed (let machines do machine-work!).\n", "That is, we need a proper lockfile.\n", "\n", "So here's my solution to this problem such that we are still able to write light-weight python-based components while taking care of dependencies as you would like in the typical software development flow.\n", "\n", "One more thing: Apart from the python dependencies, we also want to ideally have the python version itself to be consistent across components \u2013 we'll take care of that as well."]}, {"cell_type": "markdown", "id": "ace93cf2-9bf3-4d87-b936-ce70726c3b21", "metadata": {}, "source": ["# Kubeflow meets UV\n", "\n", "I will use [uv](https://docs.astral.sh/uv/) here but originally implemented this using [poetry](https://python-poetry.org/) and that also works.\n", "\n", "As we saw above the decorator `dsl.component` takes apart our python function and generates the specification for building a docker image.  \n", "We will basically write a new decorator to replace that wrapper with a fixed parameter (the python version) and with a dynamically adjusting parameter (the packages to install)."]}, {"cell_type": "markdown", "id": "5ae5db1d-0553-417f-b1bd-8bba1a83ebb5", "metadata": {}, "source": ["Let's write a function that based on the `pyproject.toml` (and `uv.lock`) file exports the dependencies as a list of strings:"]}, {"cell_type": "code", "execution_count": 17, "id": "05d86354-9f99-4a0c-85fe-688ddb3baf14", "metadata": {}, "outputs": [], "source": ["#| output: false\n", "#| code-fold: true\n", "\n", "import tempfile\n", "import functools\n", "import subprocess\n", "from pathlib import Path\n", "\n", "from kfp import dsl\n", "\n", "\n", "def run(cmd: list[str]):\n", "    try:\n", "        subprocess.check_output(cmd, text=True)\n", "    except subprocess.CalledProcessError as e:\n", "        print(e.output)\n", "        raise e"]}, {"cell_type": "code", "execution_count": null, "id": "effc29fa-b2b4-456e-8752-0dd37a0f6c63", "metadata": {}, "outputs": [], "source": ["def get_packages_to_install(group: str):\n", "    \"\"\"group: name of the dependency group, i.e, the component name\"\"\"\n", "    with tempfile.NamedTemporaryFile() as file:\n", "        run(\n", "            [\n", "                \"uv\",\n", "                \"export\",\n", "                \"--group\",\n", "                group,\n", "                \"--no-hashes\",\n", "                \"--no-annotate\",\n", "                \"--no-header\",\n", "                \"--format\",\n", "                \"requirements.txt\",\n", "                \"-o\",\n", "                file.name,\n", "            ]\n", "        )\n", "        packages = Path(file.name).read_text(\"utf-8\").splitlines()\n", "    return packages"]}, {"cell_type": "markdown", "id": "73440749-8ec6-4d08-92f0-6f95f06cf308", "metadata": {}, "source": ["In a nutshell, the function calls the external program `uv` in a subprocess to export the dependencies (the whole graph!) into a list of strings that we can pass as `packages_to_install`.\n", "\n", "Now we're ready to write our component decorator:"]}, {"cell_type": "code", "execution_count": null, "id": "e1a05e03-5be8-457e-b53b-17ebe3aa1a48", "metadata": {}, "outputs": [], "source": ["@functools.wraps(dsl.component)\n", "def custom_component(func, **kwargs):\n", "    \"\"\"\n", "    A wrapper around kfp.dsl component that infers the dependencies \n", "    based on the name the pipeline component.\n", "    IMPORTANT: The dependencies group must have the exact same name \n", "    as the component for it to work!\n", "    \"\"\"\n", "    return dsl.component(\n", "        func,\n", "        base_image=\"python:3.12-slim\",\n", "        packages_to_install=get_packages_to_install(\n", "            func.__name__, pyproject=pyproject),\n", "            **kwargs,\n", "    )"]}, {"cell_type": "markdown", "id": "e5b2c777-28cc-45ee-b069-e50625a382f8", "metadata": {}, "source": ["Notice thatt we only touch the parameters `base_image` and `packages_to_install` while passing through the rest of the `kwargs`, which allows us to preserve functionality of the original decorator."]}, {"cell_type": "markdown", "id": "0710b6f6-b89c-4d72-bca6-08e5921392f7", "metadata": {}, "source": ["After that we can use the `custom_component` decorator to define our pipeline components in a rather ergonomic fashion:"]}, {"cell_type": "code", "execution_count": null, "id": "a83d185a-dc31-4b41-82a5-2ea1ad364074", "metadata": {}, "outputs": [], "source": ["@custom_component\n", "def download_dataset(dataset: dsl.Output[dsl.Dataset]): ...\n", "    \n", "@custom_component\n", "def train(dataset: dsl.Input[dsl.Dataset], predictions: dsl.Output[dsl.Artifact]): ...\n", "\n", "@custom_component\n", "def plot_confusion_matrix(predictions: dsl.Input[dsl.Artifact]): ...\n", "\n", "@dsl.pipeline\n", "def pipeline():\n", "    dataset_task = download_dataset()\n", "    train_task = train(dataset=dataset_task.outputs[\"dataset\"])\n", "    plot_task = plot_confusion_matrix(predictions=train_task.outputs[\"predictions\"])"]}, {"cell_type": "markdown", "id": "792bc1ef-20a9-419a-96ad-48c097e5c023", "metadata": {}, "source": ["How neat is that?! \ud83d\ude80"]}, {"cell_type": "markdown", "id": "ed9d1178-6ee8-4bf0-81e2-18df1a2c8965", "metadata": {}, "source": ["# Adding a new component\n", "To add a new pipeline component we just need to add the dependency group block to the `pyproject.toml` (or simply run `uv add --group <component-name> <dependencies>` which will do that for us).  \n", "\n", "The file could like this:\n", "\n", "```toml\n", "[project]\n", "name = \"pipeline\"\n", "version = \"0.1.0\"\n", "description = \"An ML pipeline\"\n", "readme = \"README.md\"\n", "requires-python = \">=3.13\"\n", "dependencies = [\n", "    \"kfp >= 2.13.0\",\n", "]\n", "\n", "[dependency-groups]\n", "download_dataset = [\n", "    \"scikit-learn>=1.7.1\",\n", "]\n", "train = [\n", "    \"scikit-learn>=1.7.1\",\n", "    \"numpy<2.0\",\n", "]\n", "plot_confusion_matrix = [\n", "    \"matplotlib>=3.10.5\",\n", "    \"scikit-learn>=1.7.1\",\n", "    \"seaborn>=0.13.2\",\n", "]\n", "\n", "```"]}, {"cell_type": "markdown", "id": "374cee15-18e3-4691-aef7-53360fa7e1d8", "metadata": {}, "source": ["The good thing is that `uv` will automatically generate the lockfile ensuring reproducibility."]}, {"cell_type": "markdown", "id": "30bde76a-91bc-4dde-bbde-596d8a4e4f7a", "metadata": {}, "source": ["# Wrapping up\n", "\n", "I showed you a workflow that\n", "\n", " - Use standard pyproject.toml format\n", " - Keep consistent python version and dependencies across components\n", " - Manage dependencies of all components at once, including lockfile\n", " - Improve maintainability (eg. automatic security fixes) of the pipeline\n", " - Facilitate integration of new components\n", " - Preserve functionality of the native `kfp.dsl.component`\n", "\n", "You can find all the code in [this repo](https://github.com/fabridamicelli/kubeflow-pipeline-uv) with a fully working example of a pipeline that runs locally."]}, {"cell_type": "markdown", "id": "e12e34ba-338c-4be9-8750-10b3751c208e", "metadata": {}, "source": ["<div style=\"text-align: right; font-size: 40px; font-family: 'Inconsolata', monospace;\">\n", "  /Fin\n", "</div>\n", "    \n", "<div style=\"font-family: 'Inconsolata', monospace;\">\n", "Any bugs, questions, comments, suggestions? Ping me on [twitter](https://www.twitter.com/fabridamicelli) or drop me an e-mail (fabridamicelli at gmail).  \n", "Share this article on your favourite platform:\n", "</div>"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.13.1"}}, "nbformat": 4, "nbformat_minor": 5}