{
  "cells": [
    {
      "cell_type": "raw",
      "id": "a0645709",
      "metadata": {},
      "source": [
        "---\n",
        "author: Fabrizio Damicelli\n",
        "badges: false\n",
        "branch: master\n",
        "categories:\n",
        "- python\n",
        "- json\n",
        "comments: false\n",
        "date: '2024-07-26'\n",
        "description: A couple of tips to parse JSON Lines faster in Python.\n",
        "filters:\n",
        "- social-share\n",
        "output-file: 2024-07-26-parse-jsonl.html\n",
        "share:\n",
        "  description: \"\\\"Parsing JSON takes time \\u2013 time is money. A couple of tips to\\\n",
        "    \\ parse JSON Lines faster in Python.\\\"\"\n",
        "  linkedin: true\n",
        "  mastodon: true\n",
        "  permalink: '\"https://fabridamicelli.github.io/posts/2024-07-24-parse-jsonl.html\"'\n",
        "  reddit: true\n",
        "  twitter: true\n",
        "title: \"Parsing JSON takes time \\u2013 time is money\"\n",
        "toc: false\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6bf1cb7-66bf-43bf-9637-a4123e5ee5f3",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "JSON Lines is a common format encountered in modern data applications, as stated in [this documentation](https://jsonlines.org/):\n",
        "\n",
        "> The JSON Lines text format, also called newline-delimited JSON, is a convenient format for storing structured data that may be processed one record at a time. It's a great format for log files. It's also a flexible format for passing messages between cooperating processes.\n",
        "\n",
        "For example Google Platform's BigQuery exports tables in this format per default.  \n",
        "\n",
        "I will cover a few simple tips that can speed up the parsing significantly.\n",
        "I've got a file on my computer which contains a ~9600 of such JSON lines.\n",
        "\n",
        "This is a toy example, as in the real-world workloads I deal with I typically have something like 300 Million JSON Lines to process.\n",
        "So the job might spend literally hours parsing JSON ‚Äì you guessed right: and each minute running costs üí∏."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "19035a39-fef8-4589-bc95-e8a00711ed36",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9611 sample1.jsonl\n"
          ]
        }
      ],
      "source": [
        "!wc -l sample1.jsonl"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcbc691d-75c7-49cc-ab79-cf31fb1e44e4",
      "metadata": {},
      "source": [
        "Regardless of the very specific content, our data look like this:\n",
        "\n",
        "```python\n",
        "{\n",
        "    \"key1\": True\n",
        "    \"key2\": [\"hello\", \"world\"], ...\n",
        "    \"key3\": [1231123, 1234192], ...\n",
        "    \"key4\": [\"super\", \"coool\"], ... \n",
        "    \"key5\": [\"very\", \"niiice\"], ...\n",
        "      . \n",
        "      .\n",
        "      .\n",
        "}\n",
        "\n",
        "```\n",
        "where the lists can be anywhere between 8 and ~4400 length:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "42c5de45-2645-4e87-87ad-7a10be5f5cbe",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Min length: 8\n",
            "Max length: 4403\n"
          ]
        }
      ],
      "source": [
        "#| code-fold: true\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "lines = (json.loads(line) for line in Path(\"sample1.jsonl\").read_text().splitlines())\n",
        "minlen, maxlen = 1e6, 0\n",
        "for line in lines:\n",
        "    for v in line.values():\n",
        "        if isinstance(v, (list, str)):\n",
        "            if (l:= len(v)) > maxlen:\n",
        "                maxlen = l\n",
        "            if (l:= len(v)) < minlen:\n",
        "                minlen = l\n",
        "\n",
        "print(\"Min length:\", minlen)\n",
        "print(\"Max length:\", maxlen)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c192d3d-736c-43c6-8eb9-1897c7f00de9",
      "metadata": {},
      "source": [
        "::: {.callout-warning}\n",
        "\n",
        "Code benchmarks are always tricky.\n",
        "The tips I will show, I believe, apply in general. \n",
        "But still the relative differences might vary depending on several factors, so you should profile the parsers with your own data to see which variant suits your case best.  \n",
        "Also, I will assume we don't want to use schema information about the data, i.e., we want it back as as dictionary. \n",
        "\n",
        ":::"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c652ab5-711a-4781-b101-68d14e7e3bd9",
      "metadata": {},
      "source": [
        "For the sake of the comparison, we will have for each method a function that receives a file path and returns a generator of dictionaries (being each dictionary a JSON line in a file).\n",
        "We use a generator to avoid taking into account the time to create a container (e.g., a list).  \n",
        "By the way, that is already our **Tip Number 1: Consume the lines lazily (if you only need them one by one)**, to save memory footprint and time of allocating large container objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "331e6116-b320-4a4e-a966-6f902087e9db",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| code-fold: true\n",
        "from typing import Generator, Callable\n",
        "\n",
        "file = Path(\"sample1.jsonl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11f985c6-fdb0-4988-9857-1c940ac804a4",
      "metadata": {},
      "source": [
        "Here's the canonical \"pure-python\" way of doing it, using the json module from the standard library:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "72d4bef0-df68-41f8-bb83-aa5b6037ca9c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_lines_text(p: Path) -> Generator[str, None, None]:\n",
        "    \"Notice we read as text\"\n",
        "    for line in p.read_text().splitlines():\n",
        "        yield line\n",
        "        \n",
        "def read_python(p: Path, line_reader: Callable) -> Generator[dict, None, None]:\n",
        "    \"\"\"Since we do not use third-party libraries, I call this _python\"\"\"\n",
        "    for line in line_reader(p):\n",
        "        yield json.loads(line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "120deebb-e7d9-4653-8565-ed014c17b30c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def traverse_lines(lines: Generator[dict, None, None]):\n",
        "    \"\"\"\n",
        "    Go through the file, parsing the lines, but doing nothing with them.\n",
        "    We only want to parse them.\n",
        "    \"\"\"\n",
        "    for _ in lines: # Consume the generator\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "635fe1f4-934e-43c0-955d-97d379545718",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.64 s ¬± 286 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit\n",
        "traverse_lines(\n",
        "    read_python(file, line_reader=get_lines_text)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee281524-4e97-40c6-9228-088da05d5aa6",
      "metadata": {},
      "source": [
        "That's our baseline.\n",
        "\n",
        "**Tip Number 2: Consume bytes directly**\n",
        "\n",
        "Notice our function `get_lines_text` uses the method `read_text` to grab the text.\n",
        "That will under the hood first parse the bytes into a string which will then be converted into a dictionary.\n",
        "But we don't need that! \n",
        "The function `json.loads` (same as for third-party libraries) accepts bytes as well, so let's change our `get_lines` function to read bytes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "917a6b59-79d9-4557-8755-2cdcf886aa95",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_lines_bytes(p: Path) -> Generator[str, None, None]:\n",
        "    for line in p.read_bytes().splitlines():\n",
        "        yield line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "3541f9a4-71ca-4681-9b58-ed132ee8184e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.82 s ¬± 79.6 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit\n",
        "traverse_lines(\n",
        "    read_python(\n",
        "        file,\n",
        "        line_reader=get_lines_bytes  # <-- here!\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5cf6163-0991-40dc-89b7-12bfd9e2939f",
      "metadata": {},
      "source": [
        "That's ~40% improvement for free ‚Äì don't know about you, but I'd take it ;)\n",
        "\n",
        "But we can push further."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55edc116-2ac0-4012-9e7b-7675277aa145",
      "metadata": {},
      "source": [
        "**Tip Number 3: Create the stream manually (skip `pathlib`)**  \n",
        "We all love `pathlib` as it is super handy, but there's a tiny overhead in this case that can add up (if you have, say, thousands of files to read) ‚Äì let's see:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "2d326a70-450d-4f02-b9fa-4736c2e0dd64",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_lines_bytes_stream(p: Path) -> Generator[str, None, None]:\n",
        "    with open(p, \"rb\") as f:  # remember we read bytes (the default is \"rt\": read TEXT!)\n",
        "        for line in f:\n",
        "            yield line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "b7ac5ee7-482c-4f22-bc28-fedb5ca5ca24",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.65 s ¬± 13.6 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit\n",
        "traverse_lines(\n",
        "    read_python(\n",
        "        file,\n",
        "        line_reader=get_lines_bytes_stream  # <-- here!\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd6efdb9-9557-42b8-a6d3-b54302e68377",
      "metadata": {},
      "source": [
        "That's not that much of a big deal, but still, ~10% improvement, for free ‚Äì I'll take it!\n",
        "\n",
        "Starting from here, we'll see a few libraries beyond the python standard library.\n",
        "Here's also where you might want to take results with a pinch of salt, as different libraries have different implementations under the hood, which might take advantage of different aspects or structure of the data for optimization.\n",
        "Thus you definitely want to try out with your own data to check the extent to which these following results apply."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d41d49f-97eb-403e-a530-4effa4115ef1",
      "metadata": {},
      "source": [
        "**Tip Number 4: Use Pydantic's `from_json`**  \n",
        "This little function is a bit of a hidden gem.\n",
        "Almost a drop-in replacement for `json.loads`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "a4b389d1-a221-4a81-82a8-5de1dac3e931",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pydantic_core import from_json\n",
        "\n",
        "def read_pydantic(p: Path, line_reader: Callable) -> Generator[dict, None, None]:\n",
        "    for line in line_reader(p):\n",
        "        yield from_json(line, cache_strings=\"keys\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "e0920210-0c9a-4af9-b834-4e5581c4212d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.22 s ¬± 74.3 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit\n",
        "traverse_lines(\n",
        "    read_pydantic(\n",
        "        file,\n",
        "        line_reader=get_lines_bytes_stream\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9ff05f5-2245-4c73-af4e-28540be0e432",
      "metadata": {},
      "source": [
        "That's another 25% improvement (on top of what we already had so far).\n",
        "Not bad, I'd say."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6da2e0fb-a8c1-4661-94d0-6fd861697934",
      "metadata": {},
      "source": [
        "**Tip Number 5: Use `msgspec`**  \n",
        "The performance of this library has blown my mind already a few times in the past and unfortunately it lives a bit in the shadows of more visible frameworks, but I think it deserves much more attention!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "4710f53f-bfd8-4548-8af1-805476907c4a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import msgspec\n",
        "\n",
        "def read_msgspec(p: Path, line_reader: Callable) -> Generator[dict, None, None]:\n",
        "    for line in line_reader(p):\n",
        "        yield msgspec.json.decode(line)\n",
        "        # you could also instantiate the decoder outside \n",
        "        # of the function once (msgspec.json.Decoder())\n",
        "        # and call .decode() method here.\n",
        "        # For this use case, I didn't find that to have \n",
        "        # a better performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "fc522cf5-a1e4-41b8-be6b-4cd152b2a201",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "989 ms ¬± 13.4 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit\n",
        "traverse_lines(\n",
        "    read_msgspec(\n",
        "        file,\n",
        "        line_reader=get_lines_bytes_stream\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afd51f56-83f2-478b-83f0-37c11cf039e0",
      "metadata": {},
      "source": [
        "Almost 20% faster than the previous solution ‚Äì and we are still talking about almost drop-in replacements!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66e49411-c03e-458c-9ba5-ab9edc6f25d0",
      "metadata": {},
      "source": [
        "**Tip Number 6: Use `orjson`**  \n",
        "This is a popular library, claiming to be the fastest JSON parser, let's see:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "adfc1a37-2f02-4a26-9d4f-3a6137b7ecb6",
      "metadata": {},
      "outputs": [],
      "source": [
        "import orjson\n",
        "\n",
        "def read_orjson(p: Path, line_reader: Callable) -> Generator[dict, None, None]:\n",
        "    for line in line_reader(p):\n",
        "        yield orjson.loads(line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "7df7ecf4-f898-4ca3-8753-f621063c3460",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "863 ms ¬± 5.01 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit\n",
        "traverse_lines(\n",
        "    read_orjson(\n",
        "        file,\n",
        "        line_reader=get_lines_bytes_stream\n",
        "    )\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "d1bab8ba-9106-471f-87dc-877aade9a070",
      "metadata": {},
      "source": [
        "We keep stacking improvements: ~13% faster than the previous solution.\n",
        "\n",
        "To sum up, we managed to push down the parsing time from ~2.64 seconds (or 1.82 seconds if we exclude the \"naive\" case reading text) to ~0.85 seconds.\n",
        "**All in all that means more than 3 times faster!** (or more than 2 if we just read bytes).  \n",
        "A quick back-of-the-envelope calculation for my concrete, full use case results in reducing the running time by almost 10 hours, which can definitely mean some money depending on the hardware being used (for example GPUs)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7e6d3d4-473c-4aa9-9514-77d866ec96a3",
      "metadata": {},
      "source": [
        "**Bonus Tip: Use `polars`**  \n",
        "\n",
        "We only considered the case reading and parsing the lines into a dictionary.  \n",
        "But if you don't mind having the data in a [polars](https://github.com/pola-rs/polars) dataframe you can try it out:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edc8a4a8-8989-469e-87b5-414848273a01",
      "metadata": {},
      "outputs": [],
      "source": [
        "import polars as pl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "bcf145a5-7c1d-4e8f-b965-42f816e9c3d7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 4.56 s, sys: 729 ms, total: 5.29 s\n",
            "Wall time: 900 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "_ = pl.scan_ndjson(file).collect(streaming=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "baef5fff-5cd0-4f92-b2c3-bd496ec474c8",
      "metadata": {},
      "source": [
        "Here are the links to the third-party libraries.  \n",
        "Go show them some gratitude for their contributions to the community! ‚ù§Ô∏è\n",
        "\n",
        "- [pydantic-core](https://github.com/pydantic/pydantic-core)\n",
        "- [msgspec](https://github.com/jcrist/msgspec)\n",
        "- [orjson](https://github.com/ijl/orjson)\n",
        "- [polars](https://github.com/pola-rs/polars)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e12e34ba-338c-4be9-8750-10b3751c208e",
      "metadata": {},
      "source": [
        "<div style=\"text-align: right; font-size: 40px; font-family: 'Inconsolata', monospace;\">\n",
        "  /Fin\n",
        "</div>\n",
        "    \n",
        "<div style=\"font-family: 'Inconsolata', monospace;\">\n",
        "Any bugs, questions, comments, suggestions? Ping me on [twitter](https://www.twitter.com/fabridamicelli) or drop me an e-mail (fabridamicelli at gmail).  \n",
        "Share this article on your favourite platform:\n",
        "</div>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
