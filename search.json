[
  {
    "objectID": "code.html",
    "href": "code.html",
    "title": "Code",
    "section": "",
    "text": "I created and maintain the following Python packages:\nechoes: A scikit-learn compatible package for Machine Learning with Echo State Networks\n[x] A high level API for machine learning with Echo State Networks (ESN)\n[x] scikit-learn compatible implementation, powered by numba for speed.\n[x] pip install echoes\n \nkuramoto: Classical model to study synchronization phenomena\n- [x] Python implementation of the Kuramoto model on graphs.\n- [x] pip install kuramoto\n\nser: Susceptible-Excited-Refractory, a dynamical model of spreading excitations on graphs\n- [x] Powered by numba for speed.\n- [x] pip install ser\n\n\n\n\nCheck out my GitHub Profile"
  },
  {
    "objectID": "posts/2021-09-18-starmap.html",
    "href": "posts/2021-09-18-starmap.html",
    "title": "What is starmap?",
    "section": "",
    "text": "Let’s look at a common pattern in Python code:\n\nnumbers = range(10)\n\n\nlist(numbers)\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\n\ndef square(x):\n    return x**2\n\n\nresults = []\nfor n in numbers:\n    results.append(square(n))\n\n\nresults\n\n[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n\n\nThat’s fine. But Pythonistas often prefer list comprehensions like this:\n\nresults = [square(n) for n in numbers]\n\nresults\n\n[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n\n\nEquivalently, we can do that with the built in map function:\n\nlist(map(square, numbers))\n\n[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n\n\nNice. But sometimes we want to do something less trivial:\n\nresults = [\n    n**2 if n%2 == 1 else n\n    for n in numbers\n]\n\n\nresults\n\n[0, 1, 2, 9, 4, 25, 6, 49, 8, 81]\n\n\nCompact and nice, but the cognitive load starts growing. Arguably not what we want. We’d rather have a little function:\n\ndef square_if_odd(x):\n    if x % 2 == 1:\n        return x**2\n    return x\n\n\nresults = [square_if_odd(n) for n in numbers]\n\nresults\n\n[0, 1, 2, 9, 4, 25, 6, 49, 8, 81]\n\n\nEquivalently:\n\nresults = list(map(square_if_odd, numbers))\n\nresults\n\n[0, 1, 2, 9, 4, 25, 6, 49, 8, 81]\n\n\nReading a for loop triggers this voice inside our heads that kind of spells the operation. Sometimes that works great. But often times I find the map operation to reduce that cognitive load and to improve readability.\nLet’s go one step further to see what I mean:\n\nnumbers2 = range(5, 15)\nnumbers3 = range(10, 20)\n\n\nlist(numbers2), list(numbers3)\n\n([5, 6, 7, 8, 9, 10, 11, 12, 13, 14], [10, 11, 12, 13, 14, 15, 16, 17, 18, 19])\n\n\nSay we want to do something combining inputs like this:\n\ndef add_and_exp(a,b,c):\n    return (a + b) ** c\n\n\nresults = [add_and_exp(a, b, c) for a, b, c in zip(numbers, numbers2, numbers3)]\n\n\nresults\n\n[9765625,\n 1977326743,\n 282429536481,\n 34522712143931,\n 3937376385699289,\n 437893890380859375,\n 48661191875666868481,\n 5480386857784802185939,\n 630880792396715529789561,\n 74615470927590710561908487]\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBeware that zip will stop when the shortest list of numbers is exhausted.”\n\n\nThis is the equivalent using map that we would like to have.\n\nresults = map(add_and_exp, zip(numbers, numbers2, numbers3))\n\nCleaner rigth? :)\nNow, map is lazy so nothing will happen until we evaluate it:\n\nlist(results)\n\nTypeError: add_and_exp() missing 2 required positional arguments: 'b' and 'c'\n\n\nUpps!\nzip is giving us tuples that look like this (a, b, c), where a, b, c come from numbers, numbers2, numbers3, respectively.\n\nlist(zip(numbers, numbers2, numbers3))\n\n[(0, 5, 10),\n (1, 6, 11),\n (2, 7, 12),\n (3, 8, 13),\n (4, 9, 14),\n (5, 10, 15),\n (6, 11, 16),\n (7, 12, 17),\n (8, 13, 18),\n (9, 14, 19)]\n\n\nBut our function takes 3 arguments. So we would like to unpack each tuple before passing it to add_and_exp. We could modify the function to handle that. But we don’t have to, because starmap does exactly that for us:\n\nfrom itertools import starmap\n\n\nresults = starmap(add_and_exp, zip(numbers, numbers2, numbers3))\n\n\nlist(results)\n\n[9765625,\n 1977326743,\n 282429536481,\n 34522712143931,\n 3937376385699289,\n 437893890380859375,\n 48661191875666868481,\n 5480386857784802185939,\n 630880792396715529789561,\n 74615470927590710561908487]\n\n\nNow go out and write some beatiful functional Python :)\nHere’s the video version of this tutorial:\n\nReferences: - Python itertools documentation\n\nAny bugs, questions, comments, suggestions? Ping me on twitter or drop me an e-mail (fabridamicelli at gmail)."
  },
  {
    "objectID": "posts/2019-12-12-interactive-embedding.html",
    "href": "posts/2019-12-12-interactive-embedding.html",
    "title": "Does your embedding make sense?",
    "section": "",
    "text": "It’s not about the projections for the rest of 2020, I promise. Nor 2021.\nTL;DR:\n\nImagine you are working with high-dimensional data, that is, the position of each data point in that multidimensional space can be represented by a large number of other features/coordinates. For example, you measure a bunch of properties of a product where each item has some values associated, say, size, cost of production, price, CO2 footprint, etc. It could also be the case that your features are more abstract quantities. Instead of price or size, you could just have a bunch of numbers representing each item that don’t necessarily have a human meaning, for instance a vector like this [0.11, 0.34, 0.15, 0.9]. Hopefully, those number actually mean something (whatever we understand by “meaning something”). Hopefully. Things start to get trickier. A beautiful example of that are word-embeddings, which is the thing I was originally playing with when I needed the visualization that I will show you. These embeddings represent each word with a vector, typically having between 300 and 1000 dimensions. That’s what I mean by high-dimensional. What the coordinates actually mean in that case is highly non-trivial and you can find out about it in the above-mentioned article. Other datasets are even more massive in terms of the number of features though, e.g., gene expression data are usually orders of magnitude larger.\nI’d be saying nothing new if I pointed out that humans (we typically don’t talk about that with other animals) have a hard time visualizing space with more than three dimensions. Even with 86 billion of neurons between their ears. At this point, several of you will be saying: “but! Carl Sagan… bla, bla”.. In case you’re too young, too old or simple had something else better to do and didn’t watch that episode, just watch this short video and then get back – you won’t regreat:\n\nBack to the data. People do all sorts of tricks to wrap their mind around those wild, high-dimensional data in spaces with really funky shapes and fundamentally counterintuitive to our daily spatial perception of the world. Here comes a whole zoo of dimensionality reduction methods (e.g., PCA, tSNE) that project your data down to something your visual system can deal with. Of course, visualization is just one use case, but you may also want to store your data more efficiently or find outliers, or simply make life easier to your machine learning algorithm. We won’t talk about those here.\n\nWhy bother? The good, the bad and the ugly metric.\nLet’s say you have your projected data in 2D, for example, after tSNE. First things first: you might want to check if the projection that your method just spit out makes any sense at all. But, why, what can go wrong? Well, a lot. For instance, your dimensionality reduction method might be using an inappropiate metric. In other words, the algorithm could be using a notion of distance between points in the multidimensional space which does not capture some fundamental aspect of the funky space that those data points live in. Thus, further operations we do with/to our data might be strongly misleading. That is not what we want. To keep it concrete, just take a look at these nice examples from the documentation of the UMAP library, where the effect of using different metrics is clearly depicted.\nSome libraries, like UMAP, also allow you to document your embedding or plot interactive figures. But you might have a tougher luck this time around and just get a cloud of unintelligible points scattered when you plot the 2D-projected data. In that case, you might find it useful to interact with it by looking at the labels of your data. The labels can be anything, like a priori known categories the result of your favourite clustering method.\nThat’s what you came for :) So without further ado, on to the code.\n\n\nCode\nimport altair as alt\nimport pandas as pd\nfrom sklearn.datasets import load_digits\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans\n\n\ndef plot_interactive_embedding(\n    source=None,\n    x=None,\n    y=None,\n    target_col=None,\n    color_col=None,\n    alpha=0.9,\n    markersize=40,\n    grid=True,\n    max_n_show = 25,\n    figsize=(500, 500),\n    filename=None\n):\n    \"\"\"\n    Simple function for interactive visualization labels of a 2D embedding\n    (e.g., PCA projection).\n\n    Altair Chart is generated (as html), where one can brush over the scatter\n    plots and given labels are shown.\n    Color can be optionally mapped to values as well (e.g., to compare embedding\n    with a clustering method).\n\n    This is a quick adaptation of this example:\n    https://altair-viz.github.io/gallery/scatter_linked_table.html\n    \n    Parameters\n    ----------\n    source: pandas Dataframe\n        Data to plot.\n    x: str\n        Column name of x coordinate data.\n        This name will be also used as axis label.\n    y: str\n        Column name of y coordinate data.\n        This name will be also used as axis label.\n    target_col: str\n        Column name of target data, i.e., the labels to brush over.\n    color_col: str, optional. Default None.\n        Column name of data encoding color.\n        If None, all points will have same color.\n    alpha: float (0-1), optional. Default .9.\n        Opacity of points.\n    markersize: float, int, optional. Default 40.\n        Size of the points.\n    grid: bool, optional. Default True.\n        Grid in the background. Set to False to remove it.\n    max_n_show: int, optional. Dafault 25.\n        Maximum number of (target) labels to show when brushing over the points.\n    figsize: tuple (floats), optional. Default (500, 500).\n        Values for (width, height)\n    filename: str, optional. Default None.\n        If given, the chart will be saved.\n        The name must include extension - one of [.json, .html, .svg, .png].\n\n    Returns\n    -------\n    chart: Altair Chart\n        Instance of chart for further tweaking\n\n    \"\"\"\n    width, height = figsize\n    # Brush for selection\n    brush = alt.selection(type='interval')\n\n    # Scatter Plot\n    points = alt.Chart(\n        source,\n        width=width,\n        height=height\n    ).mark_circle(size=markersize).encode(\n        x=x,\n        y=y,\n        color=alt.value('steelblue')\n              if color_col is None\n              else alt.Color(color_col+\":N\", scale=alt.Scale(scheme='Spectral'))\n\n    ).add_selection(brush)\n\n    # Base chart for data tables\n    ranked_text = alt.Chart(source).mark_text().encode(\n        y=alt.Y('row_number:O',axis=None)\n    ).transform_window(\n        row_number='row_number()'\n    ).transform_filter(\n        brush\n    ).transform_window(\n        rank='rank(row_number)'\n    ).transform_filter(\n        alt.datum.rank &lt; max_n_show\n    )\n\n    # Data Tables\n    text = ranked_text.encode(text=target_col+\":N\").properties(title=target_col)\n\n    chart = alt.hconcat(\n    points,\n    text,\n    ).configure_axis(grid=grid)\n\n    if filename:\n        chart.save(filename)\n\n    return chart\n\n\n\n\nCode\n# Get digits dataset\ndigits = load_digits()\n# Cluster it with kmeans and get the predicted labels\nkmlabel = KMeans(n_clusters=10, n_jobs=-1).fit_predict(digits.data)\n# Embed in 2D with tSNE\nembedding = TSNE(n_components=2, n_iter=500).fit_transform(digits.data)\n\n# Turn data into a dataframe\ndigits_df = pd.DataFrame({\n    \"tSNE-1\": embedding[:, 0],\n    \"tSNE-2\": embedding[:, 1],    \n    \"digit\": digits.target,\n    \"KM-label\": kmlabel\n})\n\n# Make the chart\nplot_interactive_embedding(\n    source=digits_df,\n    x=\"tSNE-1\",\n    y=\"tSNE-2\",\n    target_col=\"digit\",\n    color_col=\"KM-label\",\n    figsize=(450, 450),\n    filename=\"your_filename\"\n)\n\n\n\n\n\nYou can save the output by passing a filename argument. That will generate an interactive file like this.\n\n\nAll the code can be found here.\n\n\nA couple of links worth checking out:\n\nAltair: “Declarative Visualization in Python”. This does the job under the hood of what I showed you. I did not even scratch the surface of that powerful, neat library.\nThis and this for cool visualizations of UMAP projections.\nwhatlies: “A library that tries help you to understand. What lies in word embeddings?”. This didn’t exist when I wrote this function – would be my first choice now. It goes way beyond of what I showed and has integration with other NLP tools as well.\n\nBonus: Funny games in high dimensions!\n\n\nAny bugs, questions, comments, suggestions? Ping me on twitter or drop me an e-mail (fabridamicelli at gmail)."
  },
  {
    "objectID": "posts/2019-11-30-divide-and-conquer.html",
    "href": "posts/2019-11-30-divide-and-conquer.html",
    "title": "Divide and conquer",
    "section": "",
    "text": "TL; DR: If you need to compute many vector pairwise metrics in batches, try sklearn.metrics.pairwise_distances_chunked"
  },
  {
    "objectID": "posts/2019-11-30-divide-and-conquer.html#the-problem",
    "href": "posts/2019-11-30-divide-and-conquer.html#the-problem",
    "title": "Divide and conquer",
    "section": "The problem",
    "text": "The problem\nI had to compute pairwise cosine distances for a large list of high-dimensional vectors (e.g. word embedding). After a couple of (very bad) possible solutions I found a reasonable one, of course, standing on the shoulders of giants: the sklearn function sklearn.metrics.pairwise_distances_chunked. It is pretty much a one-liner and you don’t need to care about manually splitting/parallelizing things. This is a quick write-up for other people to save that time.\nThe intuition behind the computation we want to achieve is depicted in the following plot:\n\nTwo vectors in 2D space represented as points. The blue line shows the euclidean distance between the vectors. The \\(\\cos(\\alpha)\\) is the cosine distance.\n\n\nCode\n# Import everything we need\nimport time\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom scipy.spatial.distance import cosine\nfrom sklearn.metrics.pairwise import cosine_distances\nfrom sklearn.metrics import pairwise_distances_chunked\nfrom tqdm import tqdm\nimport seaborn as sns\n\nsns.set(context=\"notebook\", font_scale=1.4, style=\"whitegrid\")\n\n\nFor the sake of presentation and simplicity, we are just going to create and use a bunch of random vectors. And that bunch is going to be relatively large (to compute on a not-so-large laptop).\nLet’s first get an idea of how many values need to compute if we take say 50 thousand vectors, which is not unrealistic at all (e.g., that could be taking all nouns of a word embedding):\n\nn_vectors = 50_000\nprint(\"Number of pairwise distances to be computed\", int(n_vectors * (n_vectors-1) / 2))\n\nNumber of pairwise distances to be computed 1249975000\n\n\nOh, that is indeed quite a few of them."
  },
  {
    "objectID": "posts/2019-11-30-divide-and-conquer.html#the-default-solution",
    "href": "posts/2019-11-30-divide-and-conquer.html#the-default-solution",
    "title": "Divide and conquer",
    "section": "The default solution",
    "text": "The default solution\nThe most straight forward to do this is with scipy/sklearn pairwise distances functions as follows (we are going to time it to get an idea of how the problem scales).\n\n%%time\n\nn_vectors = 5_000\nn_dimensions = 300   \n\nall_vectors = np.random.random(size=(n_vectors, n_dimensions))\n\n# Compute pairwise distances with function from sklearn\ndistances = cosine_distances(all_vectors)\n\nCPU times: user 1.53 s, sys: 347 ms, total: 1.88 s\nWall time: 286 ms\n\n\nSo far, so good. But what happens if we want to compute it for more vectors?\n\n%%time\n\nn_vectors = 15_000\n\nall_vectors = np.random.random(size=(n_vectors, n_dimensions))\ndistances = cosine_distances(all_vectors)\n\nCPU times: user 5.75 s, sys: 3.22 s, total: 8.97 s\nWall time: 1.99 s\n\n\nAs we see, we have 3 times more vectors, but the computation takes ~9 times longer! The reason for that is that the complexity of the problem scales non-linearly with the size of the input (number of vectors). In particular, the complexity is \\(O(n^2)\\), since we are filling the upper half of a square matrix, which grows as the square of number of vectors divided by two (which does not count for the limit case).\nIn practice, we don’t care much about those calculations as long as our computer is able to manage it in a reasonable time.\nSo how about having more vectors, say 30 or 50 thousand?\n\nCloser look at the scaling\nLet’s examine the computational complexity we mentioned above with some quick numerical experiments.\n\n\nCode\ndef evaluate_scaling(func, n_vectors_vals, n_dimensions=300):\n    \"\"\"Return times of func execution for n_vectors_vals\"\"\"\n    times = []\n    for n_vectors in tqdm(n_vectors_vals):\n        all_vectors = np.random.random(size=(n_vectors, n_dimensions))  \n        st = time.time()\n        distances = func(all_vectors)\n        times.append(time.time()-st)\n        del distances\n    return times\n\ndef plot_scaling(n_vectors_vals, times, **kwargs):    \n    plt.plot(n_vectors_vals, times, linewidth=3, alpha=.8, **kwargs)\n    plt.xlabel(\"Number of vectors\")\n    plt.ylabel(\"Time elapsed (seconds)\")\n    plt.grid(True, linestyle=\"--\", alpha=0.7)\n\n\n\nn_vectors_vals = np.arange(1000, 20001, 500)\ntimes = evaluate_scaling(cosine_distances, n_vectors_vals)\n\n100%|██████████| 39/39 [00:49&lt;00:00,  1.27s/it]\n\n\n\nplot_scaling(n_vectors_vals, times)\n\n\n\n\nWe can use what we know about the complexity (\\(O(n^2)\\)) to fit a curve. In other words, we are going to fit a quadratic function that predicts the time it takes to compute all the pairwise distances as a function of the number of vectors. After that, we can use that function to extrapolate and estimate the performance for a much larger number of vectors.\n\n\n# Fit a 2nd degree polynomial and get the polynomial evaluator\nfit = np.polyfit(n_vectors_vals, times, 2)  \npoly = np.poly1d(fit)\n\n# Check our fit\nplot_scaling(n_vectors_vals, times, label=\"Actual\")\nplot_scaling(n_vectors_vals, poly(n_vectors_vals), label=\"Quadratic Fit\")\nplt.legend();\n\n\n\n\nThe fit looks approximately correct. Remember, we don’t need to get a perfectly accurate estimate. We rather want to know if we should grab a coffee while the computation runs, let it compute overnight or if it is unfeasible with our hardware.\nNow we extrapolate for more vectors:\n\nn_vectors_large = np.arange(1000, 50000)\nplot_scaling(n_vectors_large, poly(n_vectors_large))\n\n\n\n\nWell that doesn’t sound too bad: it should take around 40 seconds to compute the distances for 50 thousand vectors.  Let’s give it a try:\n\nn_vectors = 50_000\n\nall_vectors = np.random.random(size=(n_vectors, n_dimensions))\ndistances = cosine_distances(all_vectors)\n\n[I 15:19:07.791 LabApp] KernelRestarter: restarting kernel (1/5), keep random ports\nkernel 4055ff16-c49b-4a76-9068-e899d001fb85 restarted\nUps! We’ve blown up the memory and forced the kernel to commit suicide.  If you’re running the code along and 50 thousand still works in your computer, just try a higher number, you’ll get there pretty soon. My machine is fine until ~30 thousand vectors.\nIn short, what we thought was our initial problem (computation time) is actually secondary (it would take less than a minute). But either the result itself (cosine distances matrix) or other structures during intermediate computations simply don’t fit in memory."
  },
  {
    "objectID": "posts/2019-11-30-divide-and-conquer.html#solution-first-attempt",
    "href": "posts/2019-11-30-divide-and-conquer.html#solution-first-attempt",
    "title": "Divide and conquer",
    "section": "Solution: first attempt",
    "text": "Solution: first attempt\nIterate and compute the values one by one instead of computing it with the cosine_distances function.  Spoiler: Bad idea.  Let’s see:\n\n\nCode\ndef cosine_distances_iter(all_vectors):\n    n_vectors = all_vectors.shape[0]\n    distances = np.zeros((n_vectors, n_vectors))\n    # D is symmetric, so we don't want to compute twice - just use upper diag indices\n    for i, j in zip(*np.triu_indices(n_vectors, k=1)):\n        distances[i, j] = cosine(all_vectors[i], all_vectors[j])\n    return distances\n\n\n\nn_vectors = 100\nall_vectors = np.random.random(size=(n_vectors, n_dimensions))\ndistances = cosine_distances_iter(all_vectors)\n\nAs the distance matrix is symmetric, we don’t repeat the computation and thus here we just show the upper triangle. Each entry \\(D_{ij}\\) of the distance matrix corresponds to the cosine distance between the vectors \\(i\\) and \\(j\\).\n\n\nCode\ndef plot_distances_heatmap(distances):    \n    sns.heatmap(\n        distances, \n        mask=~np.triu(distances).astype(bool), \n        cbar_kws={\"label\": \"Cosine distance\"},\n        cmap=\"magma\",\n        square=True, \n        xticklabels=False, \n        yticklabels=False,\n    )\n    plt.title(\"Distance Matrix\")\n    plt.show()\n\n\n\nplot_distances_heatmap(distances)\n\n\n\n\nNow, how does it scale? We can do the same curve fitting as above and project for a larger number of vectors.\n\n\nCode\nn_vectors_vals = np.arange(100, 1001, 100)\ntimes = evaluate_scaling(cosine_distances_iter, n_vectors_vals)\n\n# Fit a 2nd degree polynomial and get the polynomial evaluator\nfit = np.polyfit(n_vectors_vals, times, 2)  \npoly = np.poly1d(fit)\n\n# Check our fit\nplot_scaling(n_vectors_vals, times, label=\"Actual\")\nplot_scaling(n_vectors_vals, poly(n_vectors_vals), label=\"Quadratic Fit\")\nplt.legend();\n\n\n100%|██████████| 10/10 [01:03&lt;00:00,  6.36s/it]\n\n\n\n\n\nAnd the extrapolation looks like this:\n\n\nCode\nn_vectors_large = np.arange(100, 50001, 100)\nplot_scaling(n_vectors_large, poly(n_vectors_large))\n\n\n\n\n\nThat is going to take a while – way longer than grabbing a coffee.  Conclusion: there must be a better way!"
  },
  {
    "objectID": "posts/2019-11-30-divide-and-conquer.html#same-giants-same-shoulders-scikit-learn-to-the-rescue",
    "href": "posts/2019-11-30-divide-and-conquer.html#same-giants-same-shoulders-scikit-learn-to-the-rescue",
    "title": "Divide and conquer",
    "section": "Same giants, same shoulders: scikit-learn to the rescue",
    "text": "Same giants, same shoulders: scikit-learn to the rescue\nA much better alternative was to look into the scikit-learn library.  It turns out there is a function pairwise_distances_chunked, which does exactly what we want. As the documentation explains, this function creates a Python generator that will build up a distance matrix chunk by chunk, thus computing the distances as lazily and returning the intermediate results. The following example will be hopefully useful:\n\nn_vectors = 20_000\nn_dimensions = 100 \n\nall_vectors = np.random.random(size=(n_vectors, n_dimensions))\n\n# We create an empty placeholder for the results, so that we \n# can visualize the intermediate steps\ndistances = np.zeros((n_vectors, n_vectors))\n\nInstead of computing and storing all the results, we construct the generator first:\n\nchunk_generator = pairwise_distances_chunked(all_vectors, metric=\"cosine\")\ntype(chunk_generator)\n\ngenerator\n\n\nNow we can call the next method and so generate the first chunk of results.  Like with any other Python generator, we can repeat that call until the generator is exhausted.\n\nchunk1 = next(chunk_generator)\nprint(\"Shape of chunk 1:\", chunk1.shape)\nchunk2 = next(chunk_generator)\nprint(\"Shape of chunk 2:\", chunk2.shape)\nchunk3 = next(chunk_generator)\nprint(\"Shape of chunk 3:\", chunk3.shape)\n\nprint(\"Total size along first dimension :\", sum((chunk1.shape[0], chunk2.shape[0], chunk3.shape[0])))\n\nShape of chunk 1: (6710, 20000)\nShape of chunk 2: (6710, 20000)\nShape of chunk 3: (6580, 20000)\nTotal size along first dimension : 20000\n\n\nAs we observe on the shape of the generated results, the rendered chunk is a vertical slice of the complete distance matrix.  We can visualize it (yellow corresponds to the computed values):\n\n\nCode\n# WARNING: running this cell might take quite a bit of memory\n\nchunk_generator = pairwise_distances_chunked(all_vectors, metric=\"cosine\")\n\nfig, axes = plt.subplots(ncols=3, figsize=(15, 4))\nchunks_idx = range(1, 4)  # this depends on the number of total chunks (which I happen to know is 3 here)\ncurrent_row = 0\ndistances = np.zeros((n_vectors, n_vectors))\nfor ax, c in zip(axes.flat, chunks_idx):\n    chunk = next(chunk_generator)\n    n_rows, _ = chunk.shape\n    # Update distances matrix\n    distances[current_row: current_row + n_rows, :] = chunk\n    current_row += n_rows    \n    ax.imshow(distances, cmap=\"RdYlBu_r\")        \n    ax.set_title(f\"Distance Matrix after chunk{c}\", fontsize=15)\n    ax.grid(False)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe size of each chunk will be figured out automatically by scikit-learn, no need to worry about that. If the results are small enough, it might just dispatch it all in one batch.\n\n\n\nBack to 50K\nSo far so good. But our original problem was substantially larger than the example above, namely 50 (not 20) thousand vectors, which we already saw translates into many more computations. \nNow we are going to test the chunked approach with more vectors – that’s what you came for :)\n\nn_vectors = 50_000\nn_dimensions = 300 \nall_vectors = np.random.random(size=(n_vectors, n_dimensions))\n\nchunk_generator = pairwise_distances_chunked(all_vectors, metric=\"cosine\")\n\nAssuming the final whole array fits in memory, we could collect all chunks and then concatenate them, like this:\ndistances = np.vstack(chunk_generator)\nwhich is very nice, but will stop working with numpy 1.16, thus we need another container\n\n# This might take a lot of RAM, so depending on your hardware you might just skip the concatenation\ndistances = np.vstack([chunk for chunk in chunk_generator])\nprint(\"distances shape: \", distances.shape)\n\ndistances shape:  (50000, 50000)\n\n\nVoilà!  We just computed the pairwise cosine distance for the 50 thousand vectors! If your matrix distance is too big such that cannot be concatenated into one array, then you can simply do whatever you need to with the individual chunks and save the intermediate results.\nFor the sake of completeness, let’s evaluate the scaling of that function:\n\n\nCode\ndef cosine_distance_chunks(all_vectors):\n    chunk_generator = pairwise_distances_chunked(all_vectors, metric=\"cosine\")\n    return np.vstack([chunk for chunk in chunk_generator])  \n\n\n\nn_vectors_vals = np.arange(10_000, 50_001, 10_000)\ntimes = evaluate_scaling(cosine_distance_chunks, n_vectors_vals)\n\nplot_scaling(n_vectors_vals, times)\n\n100%|██████████| 5/5 [01:10&lt;00:00, 14.12s/it]\n\n\n\n\n\n\n\nSanity check: compare to pure sklearn function\n\nn_vectors = 30_000\nn_dimensions = 300 \nall_vectors = np.random.random(size=(n_vectors, n_dimensions))\n\ndistances_skl = cosine_distances(all_vectors)\ndistances_chunk = cosine_distance_chunks(all_vectors)\n\nnp.allclose(distances_skl, distances_chunk)\n\nTrue\n\n\n\n\n\n\n\n\nTip\n\n\n\npairwise_distances_chunked has some parameters that can be pretty useful:  - n_jobs: distribute the computation across cores (though you might want to experiment a bit since overhead might make it actually worse).  - metric: choose a metric different from cosine distance[1], such as euclidean distance or even your own defined function.  You can check the rest of them in the documentation. \n\n\n[1]: The sharp eye might have noticed that the term “metric” is not quite correct here. Strictly speaking cosine distance is not a metric (the reason for that can be found here).\nTake home message: Whenever you find yourself carrying out an data/machine learning task and you have the feeling that there must be a better way, check scikit-learn first. The odds that you’ll find something useful are really on your side.\nFin\nReferences: - xkcd comic - extrapolation\n\nAny bugs, questions, comments, suggestions? Ping me on twitter or drop me an e-mail (fabridamicelli at gmail)."
  },
  {
    "objectID": "posts/2021-10-10-merge_dicts.html",
    "href": "posts/2021-10-10-merge_dicts.html",
    "title": "Merging Python dictionaries: A functional take",
    "section": "",
    "text": "Merging dictionaries\nSay we have these two dictionaries that we would like to merge:\n\nd1 = {\"a\": 1, \"b\": 2}\nd2 = {\"a\": 2, \"c\": 3, \"d\": 4}\n\nA kind of cannonical way to do it would be this:\n\nd3 = {}\nfor d in [d1, d2]:\n    for k, v in d.items():\n        d3[k] = v\n\nd3\n\n{'a': 2, 'b': 2, 'c': 3, 'd': 4}\n\n\n\n\n\n\n\n\nWarning\n\n\n\nNotice that we are updating the items, so later appeared keys will overwrite the values under existing keys.\n\n\nThat works. But it’s arguably not so nice. Here’s an alternative:\n\nd3 = {k: v for d in [d1, d2] for k, v in d.items()}\n\nd3\n\n{'a': 2, 'b': 2, 'c': 3, 'd': 4}\n\n\nThat’s compact and kind of nice because of the dictionary comprehension. But, as Michael Kennedy puts it in this video, it’s a bit of a “too clever” alternative, that might be not so easy to read.\nWhat many people consider to be a “more pythonic” way is the following:\n\nd3 = {**d1, **d2}\nd3\n\n{'a': 2, 'b': 2, 'c': 3, 'd': 4}\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThis will only work from Python 3.5 on.\n\n\nBeautiful.\nThat’s where most tutorials on merging dictionaries in Python end. Let’s go beyond that. What if we have more dictionaries to merge, say, three:\n\nd1 = {\"a\": 1, \"b\": 2}\nd2 = {\"a\": 2, \"c\": 3, \"d\": 4}\nd3 = {\"c\": 3, \"f\": 6, \"g\": 9}\n\n\nd4 = {**d1, **d2, **d3}\nd4\n\n{'a': 2, 'b': 2, 'c': 3, 'd': 4, 'f': 6, 'g': 9}\n\n\nQuestion: And how about having 10 thousand dictionaries? Or not even knowing how many you have?\nAnswer: Let’s get functional! :)\nWe can easily extend the logic of what we’ve been doing so far with one functional concept: reduce (aka “fold” in other languages).\n\n\nDetour: What is reduce\nIf you’re alredy familiar with this concept, jumpt to the next subsection.\nYou can check the details for yourself if you’re not yet familiar with the concept, eg this nice Real Python’s tutorial. In essence reduce is a higher order function that will recursively apply a combining operation to the elements of an iterable. That’s mouthful, let’s look at a couple of quick examples:\n\nfrom functools import reduce\nfrom operator import add, pow\n\n\nadd??\n\n\nSignature: add(a, b, /)\nDocstring: Same as a + b.\nType:      builtin_function_or_method\n\n\n\n\n\npow??\n\n\nSignature: pow(a, b, /)\nDocstring: Same as a ** b.\nType:      builtin_function_or_method\n\n\n\n\nBoth add and pow take two arguments and return one, so they “reduce” (or “fold”) the two inputs into one.\n\nreduce(add, (1,2,3,4))\n\n10\n\n\n\n1+2+3+4\n\n10\n\n\n\nreduce(pow, (2, 3, 4, 5))\n\n1152921504606846976\n\n\n\n((2 ** 3) ** 4) ** 5   # notice the succesive (recursive) nature of the operation\n\n1152921504606846976\n\n\n\n\nBack to dictionaries\nLet’s apply that to dictionary merging:\n\ndef merge(d1, d2):\n    \"\"\"Return a new dictionary which results from merging d1 and d2\"\"\"\n    return {**d1, **d2}\n\nSo far nothing new. But notice that we now have an operation that takes two arguments and returns one, in other words a “reducing” or “folding” operation, so we can now use that!\n\nd1 = {\"a\": 1, \"b\": 2}\nd2 = {\"a\": 2, \"c\": 3, \"d\": 4}\nd3 = {\"c\": 4, \"f\": 6, \"g\": 9}\n\n\nreduce(merge, (d1, d2, d3))\n\n{'a': 2, 'b': 2, 'c': 4, 'd': 4, 'f': 6, 'g': 9}\n\n\nEven some nice non-trivial properties come for free, eg it does the right thing when passing only one argument:\n\nreduce(merge, (d1,))   # notice that d1 it has to be in an iterable\n\n{'a': 1, 'b': 2}\n\n\nI think that is nice. But it can get nicer, because we can put all that together and by exploiting the arbitrary positional arguments (aka *args) we make it more general:\n\ndef merge_dicts(*dicts):\n    return reduce(lambda d1,d2: {**d1, **d2},  dicts)\n\nNow we can use the very same function to merge as many dictionaries as we’d like, just passing them as positional arguments:\n\nmerge_dicts(d1)\n\n{'a': 1, 'b': 2}\n\n\n\nmerge_dicts(d1, d2)\n\n{'a': 2, 'b': 2, 'c': 3, 'd': 4}\n\n\n\nmerge_dicts(d1, d2, d3, d1)\n\n{'a': 1, 'b': 2, 'c': 4, 'd': 4, 'f': 6, 'g': 9}\n\n\nHow cool is that? :)\nHere’s the video version of this tutorial\n\nFin\nEdit:\nIf you are using Python &gt;= 3.9, there are a couple of better alternatives:\nThe first, more compact:\n\nimport operator\n\ndef merge_dicts(*dicts):\n    return reduce(operator.__or__, dicts)\n\n\nmerge_dicts_or(d1,d2,d3)\n\n{'a': 2, 'b': 2, 'c': 4, 'd': 4, 'f': 6, 'g': 9}\n\n\nThe second, more readable:\n\ndef merge_dicts(*dicts):\n    ret = {}\n    for d in dicts:\n        ret |= d\n    return ret\n\n\nmerge_dicts_or(d1,d2,d3)\n\n{'a': 2, 'b': 2, 'c': 4, 'd': 4, 'f': 6, 'g': 9}\n\n\nBoth were pointed out by Anthony Sottile in this tweet - thanks!:\n\nReferences:\n- Python functools documentation\n- Michael Kennedy’s tutorial\n- Real Python’s article on reduce\n\nAny bugs, questions, comments, suggestions? Ping me on twitter or drop me an e-mail (fabridamicelli at gmail)."
  },
  {
    "objectID": "posts/2023-09-13-pytorch-dataloader-collate.html",
    "href": "posts/2023-09-13-pytorch-dataloader-collate.html",
    "title": "PyTorch DataLoader: Understand and implement a custom collate function",
    "section": "",
    "text": "This post contains the code behind this video explanation:\nCode\nimport torch\nfrom torch import tensor\nimport numpy as np\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.nn.utils.rnn import pad_sequence\nImagine a supervised learning scenario of a classification task with sequential data as features and a binary target.\nLet’s simulate a toy dataset and take a look at it:\nCode\nclass CustomDataset(torch.utils.data.Dataset):\n    def __init__(self):\n        self.xs = [\n            list(range(11, 13)),\n            list(range(13, 16)),\n            list(range(16, 21)),\n            list(range(21, 24)),\n            list(range(22, 25)),\n            list(range(25, 30)),\n        ]\n        self.ys = [0, 0, 0, 1, 1, 1]\n        assert len(self.xs) == len(self.ys)\n    def __len__(self): \n        return len(self.xs)\n    def __getitem__(self, idx):\n        return {\n            \"x\": self.xs[idx],\n            \"y\": self.ys[idx],\n        }\ndset = CustomDataset()\n\nfor item in dset:\n    print(item)\n\n{'x': [11, 12], 'y': 0}\n{'x': [13, 14, 15], 'y': 0}\n{'x': [16, 17, 18, 19, 20], 'y': 0}\n{'x': [21, 22, 23], 'y': 1}\n{'x': [22, 23, 24], 'y': 1}\n{'x': [25, 26, 27, 28, 29], 'y': 1}\ndloader = DataLoader(dset, batch_size=2, shuffle=False)\nfor batch in dloader:\n    print(batch)\n\nRuntimeError: each element in list of batch should be of equal size"
  },
  {
    "objectID": "posts/2023-09-13-pytorch-dataloader-collate.html#a-first-solution-attempt",
    "href": "posts/2023-09-13-pytorch-dataloader-collate.html#a-first-solution-attempt",
    "title": "PyTorch DataLoader: Understand and implement a custom collate function",
    "section": "A first solution attempt",
    "text": "A first solution attempt\nWe can refactor our dataset and make it generate items with x sequences that all have the same length (a parameter max_len that we define beforehand).\n\nclass CustomDatasetFixLen(torch.utils.data.Dataset):\n    def __init__(self, max_len=10):\n        self.max_len = max_len\n        self.xs = [\n            list(range(11, 13)),\n            list(range(13, 16)),\n            list(range(16, 21)),\n            list(range(21, 24)),\n            list(range(22, 25)),\n            list(range(25, 30)),\n        ]\n        self.ys = [0, 0, 0, 1, 1, 1]\n        assert len(self.xs) == len(self.ys)\n    def __len__(self): \n        return len(self.xs)\n    def __getitem__(self, idx):\n        x = self.xs[idx]\n        pad_len = self.max_len - len(x)\n        x = x + [0]*pad_len\n        return {\n            \"x\": np.array(x),\n            \"y\": self.ys[idx],\n        }\n\n\ndset = CustomDatasetFixLen(max_len=10)\n\n\nfor item in dset:\n    print(item)\n\n{'x': array([11, 12,  0,  0,  0,  0,  0,  0,  0,  0]), 'y': 0}\n{'x': array([13, 14, 15,  0,  0,  0,  0,  0,  0,  0]), 'y': 0}\n{'x': array([16, 17, 18, 19, 20,  0,  0,  0,  0,  0]), 'y': 0}\n{'x': array([21, 22, 23,  0,  0,  0,  0,  0,  0,  0]), 'y': 1}\n{'x': array([22, 23, 24,  0,  0,  0,  0,  0,  0,  0]), 'y': 1}\n{'x': array([25, 26, 27, 28, 29,  0,  0,  0,  0,  0]), 'y': 1}\n\n\nThat works but is wasteful because we will be padding to max_len = 10, even when we only need to pad to length 3 (for example, if the batch is formed by the first two items). That could limit the batch size we work with slowing down the training or even lead to unnecessary computations during the forward pass if we just pass our batches without masking. So, ideally, we would like to pad only as much as we need on each batch. In other words, we want to dynamically (per batch basis) adapt the padding."
  },
  {
    "objectID": "posts/2023-09-13-pytorch-dataloader-collate.html#there-must-be-a-better-way",
    "href": "posts/2023-09-13-pytorch-dataloader-collate.html#there-must-be-a-better-way",
    "title": "PyTorch DataLoader: Understand and implement a custom collate function",
    "section": "There must be a better way",
    "text": "There must be a better way\nLet’s implement our own collate function, i.e. the logic to put items together, that will allow us to the padding on a per batch basis (thus we call it dynamic_length_collate)\n\ndef dynamic_length_collate(batch):\n    max_len = max(len(item[\"x\"]) for item in batch)\n    batch_x = []\n    for item in batch:\n        pad_len = max_len - len(item[\"x\"])\n        batch_x.append(item[\"x\"] + [0]*pad_len)\n    return {\n        \"x\": tensor(batch_x).type(torch.float),\n        \"y\": tensor([item[\"y\"] for item in batch])\n    }\n\n\ndset = CustomDataset()  # Use our original dataset, without fix max_len\ndloader = DataLoader(dset, batch_size=2, shuffle=False,\n                     collate_fn=dynamic_length_collate)\n\n\nfor batch in dloader:\n    print(batch)\n\n{'x': tensor([[11., 12.,  0.],\n        [13., 14., 15.]]), 'y': tensor([0, 0])}\n{'x': tensor([[16., 17., 18., 19., 20.],\n        [21., 22., 23.,  0.,  0.]]), 'y': tensor([0, 1])}\n{'x': tensor([[22., 23., 24.,  0.,  0.],\n        [25., 26., 27., 28., 29.]]), 'y': tensor([1, 1])}\n\n\nThat works!\nFor the sake of completeness, let’s use our dataloader with the custom collate function and actually feed the data into a (toy) neural network.\n\n# A very toy example of a neural network\nmodel = torch.nn.LSTM(input_size=1, hidden_size=2, batch_first=True)\n\nfor batch in dloader:\n    bs, seq_len = batch[\"x\"].shape\n    pred = model(batch[\"x\"].reshape(bs, seq_len, 1))\n    print(pred)\n    break\n\n(tensor([[[ 9.4607e-04,  4.0929e-03],\n         [ 5.0468e-04,  5.7644e-03],\n         [-1.5826e-01,  1.9474e-02]],\n\n        [[ 2.6432e-04,  2.6775e-03],\n         [ 1.3929e-04,  3.5764e-03],\n         [ 7.2860e-05,  3.3866e-03]]], grad_fn=&lt;TransposeBackward0&gt;), (tensor([[[-1.5826e-01,  1.9474e-02],\n         [ 7.2860e-05,  3.3866e-03]]], grad_fn=&lt;StackBackward0&gt;), tensor([[[-0.2420,  0.0843],\n         [ 0.0071,  1.2391]]], grad_fn=&lt;StackBackward0&gt;)))"
  },
  {
    "objectID": "posts/2023-09-13-pytorch-dataloader-collate.html#fin",
    "href": "posts/2023-09-13-pytorch-dataloader-collate.html#fin",
    "title": "PyTorch DataLoader: Understand and implement a custom collate function",
    "section": "Fin",
    "text": "Fin\n\nAny bugs, questions, comments, suggestions? Ping me on twitter or drop me an e-mail (fabridamicelli at gmail)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Fabrizio Damicelli",
    "section": "",
    "text": "Cut through the hype. One commit first. Then another. Intuitions. That’s the game about. Let them be solid. Don’t stop learning. Remain skeptical, especially of my code. Neural nets are cool. But software and communication skills take you a longer way. AGI is cool. After six. Before that, let’s leverage data and computing power to build a better world."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "PyTorch DataLoader: Understand and implement a custom collate function\n\n\n\n\n\nUnderstand DataLoader’s inner workings and bring your data pipeline to the next level.\n\n\n\n\n\n\n2023-09-13\n\n\n\n\n\n\n  \n\n\n\n\nseaborn and tensors: A match not quite made in heaven\n\n\n\n\n\nAutopsy of a little gotcha plotting tensors data.\n\n\n\n\n\n\n2022-08-04\n\n\n\n\n\n\n  \n\n\n\n\nfrom collections import ChainMap\n\n\n\n\n\nAn elegant solution to efficiently carry out a look up over more than one dictionary.\n\n\n\n\n\n\n2022-01-29\n\n\n\n\n\n\n  \n\n\n\n\nLinear Regression: Don’t forget your bias\n\n\n\n\n\nDefaults matter. This one tricks more people than you think.\n\n\n\n\n\n\n2021-10-16\n\n\n\n\n\n\n  \n\n\n\n\nMerging Python dictionaries: A functional take\n\n\n\n\n\nAn elegant way to merge many dictionaries in Python by leveraging some functional tools.\n\n\n\n\n\n\n2021-10-10\n\n\n\n\n\n\n  \n\n\n\n\nWhat is starmap?\n\n\n\n\n\nA quick explanation of this useful functionality from the Python itertools module.\n\n\n\n\n\n\n2021-09-18\n\n\n\n\n\n\n  \n\n\n\n\nAchtung: Watch out, German csv readers!\n\n\n\n\n\nLittle pandas Gotcha dealing with German text.\n\n\n\n\n\n\n2020-05-20\n\n\n\n\n\n\n  \n\n\n\n\nExplicit is better than implicit\n\n\n\n\n\nA basic but not so obvious numpy example.\n\n\n\n\n\n\n2020-05-10\n\n\n\n\n\n\n  \n\n\n\n\nDoes your embedding make sense?\n\n\n\n\n\nProjecting data is easy. Understanding the outcome, less so. Interactive plots can help.\n\n\n\n\n\n\n2019-12-12\n\n\n\n\n\n\n  \n\n\n\n\nDivide and conquer\n\n\n\n\n\nHow to compute over a billion cosine distances on a laptop. Standing on the shoulders of the same giants, once again.\n\n\n\n\n\n\n2019-11-30\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "til.html",
    "href": "til.html",
    "title": "TIL: Today I learned",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "My PhD research was centered around two main topics:\n- By means of computational analysis and modeling, trying to understand how the connectivity of brain networks emerge.\n- The cross-talk between biological and artificial neural networks, i.e., how real brain connectivity could be integrated into artificial neural networks.\nSelected publications:\n- Topological reinforcement as a principle of modularity emergence in brain networks. Damicelli, Hilgetag, Hütt, Messé. Network Neuroscience, 2019.\n-Modular topology emerges from plasticity in a minimalistic excitable network model. Damicelli, Hilgetag, Hütt, Messé. Chaos, 2017.\n-Brain Connectivity meets Reservoir Computing. Damicelli, Hilgetag, A Goulas, 2021.\n-Bio-instantiated recurrent neural networks: Integrating neurobiology-based network topology in artificial networks. Goulas, Damicelli, Hilgetag, Neural Networks, 2021.\nCheck out Google Scholar for a complete list."
  },
  {
    "objectID": "posts/2022-08-04-seaborn-tensors.html",
    "href": "posts/2022-08-04-seaborn-tensors.html",
    "title": "seaborn and tensors: A match not quite made in heaven",
    "section": "",
    "text": "seaborn makes our life easy when it comes to slicing and plotting data in Python.\nThat awesome buffet of well balanced aesthetic and practical functionalities of its ergonomic API comes with a few caveats to consider though.\nHere’s one of them when trying to plot data including PyTorch tensors.\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom torch import tensor\nsns.set_context(context=\"talk\")\nLet’s create 2 simple numpy arrays simulating the values of two variables, x and y.\nx = np.arange(20) + np.random.normal(scale=1.7, size=20).round(2)\ny = np.arange(20) + np.random.normal(scale=1.7, size=20).round(2)\nx[:5], y[:5]\n\n(array([-1.41,  3.97,  3.2 ,  4.63,  2.99]),\n array([-0.58,  1.07, -0.13,  0.79,  3.84]))\nWe can plot them using seaborn scatterplot:\nsns.scatterplot(x=x, y=y);\nWe can achieve the same using lists:\nx_li = x.tolist()\ny_li = y.tolist()\nx_li[:5], y_li[:5]\n\n([3.93, 1.6400000000000001, 2.91, 2.75, 0.3900000000000001],\n [-1.08, 0.91, 4.970000000000001, 3.64, 8.129999999999999])\nAgain, we can plot them using seaborn scatterplot:\nsns.scatterplot(x=x_li, y=y_li);\nObserve that the lists are made up of python floats while the numpy arrays contain numpy.float64:\ntype(x[0]), type(x_li[0])\n\n(numpy.float64, float)\nSo far so good.\nNow what happens if the individual elements are pytorch zero-dimensional tensors (i.e. scalars) like these:\nx_pt = [tensor(o) for o in x] \ny_pt = [tensor(o) for o in y] \n\nx_pt[:5], y_pt[:5]\n\n([tensor(3.9300, dtype=torch.float64),\n  tensor(1.6400, dtype=torch.float64),\n  tensor(2.9100, dtype=torch.float64),\n  tensor(2.7500, dtype=torch.float64),\n  tensor(0.3900, dtype=torch.float64)],\n [tensor(-1.0800, dtype=torch.float64),\n  tensor(0.9100, dtype=torch.float64),\n  tensor(4.9700, dtype=torch.float64),\n  tensor(3.6400, dtype=torch.float64),\n  tensor(8.1300, dtype=torch.float64)])\nAt first glance it looks like it should be all kind of the same. Indeed some comparisons still work the way we (I?) expect. For example, the element-wise equality:\nx == x_li\n\narray([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True])\nx == x_pt\n\narray([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True])\n(x == x_pt).all()\n\nTrue\nx_pt == x\n\narray([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True])\nThis happens thanks to the fact that numpy under the hood first casts the objects and then compares them.\nWhen comparing the python list directly to the tensor we have this:\nx_li == x_pt\n\nTrue\nThat is not quite what I expect (i.e. element-wise comparison), but it is still aligned with our believe that these array-like structures (list, array, tensor) are made of equivalent scalar elements.\nSo let’s plot the tensors, like we did with the lists and the arrays:\nsns.scatterplot(x=x_pt, y=y_pt);\nUps, that doesn’t look good – the y-axis is flipped!\nAfter going a bit down the rabbit hole of seaborn and pandas error traces, we see that under the hood seaborn infers the data type of the values and –surprise!– pytorch tensors seem to be interpreted as categorical.\nThat can be more explicitely seen if we try to plot the data with the pointplot function instead:\nsns.pointplot(x=x_pt, y=y_pt);\n\nTypeError: Neither the `x` nor `y` variable appears to be numeric.\nTo go a bit deeper understanding this behaviour you can read the section “Categorical plots will always be categorical” of this article by seaborn’s creator Michaels Waskom himself."
  },
  {
    "objectID": "posts/2022-08-04-seaborn-tensors.html#lets-fix-it",
    "href": "posts/2022-08-04-seaborn-tensors.html#lets-fix-it",
    "title": "seaborn and tensors: A match not quite made in heaven",
    "section": "Let’s fix it",
    "text": "Let’s fix it\nA couple of options to fix our plots.\nWe can flip the y-axis:\n\nax = sns.scatterplot(x=x_pt, y=y_pt)\nax.invert_yaxis()\n\n\n\n\nWe can cast the data, for example to a numpy array:\n\nsns.scatterplot(x=np.array(x_pt), y=np.array(y_pt));\n\n\n\n\nOr to a tensor:\n\nsns.scatterplot(x=tensor(x_pt), y=tensor(y_pt));\n\n\n\n\n\nNote:\n\n# This is the state as of today with \nsns.__version__\n\n'0.11.2'\n\n\nMaybe that changes in in the future.\nNow we can go back and keep using the great seaborn library to make delightful plots."
  },
  {
    "objectID": "posts/2022-08-04-seaborn-tensors.html#fin",
    "href": "posts/2022-08-04-seaborn-tensors.html#fin",
    "title": "seaborn and tensors: A match not quite made in heaven",
    "section": "Fin",
    "text": "Fin\nPS: Consider starring the seaborn project on github.\n\nAny bugs, questions, comments, suggestions? Ping me on twitter or drop me an e-mail (fabridamicelli at gmail)."
  },
  {
    "objectID": "posts/2020-05-10-numpy-array-in-place.html",
    "href": "posts/2020-05-10-numpy-array-in-place.html",
    "title": "Explicit is better than implicit",
    "section": "",
    "text": "TL; DR: Only use the form array *= something if you’re 100% sure you are doing the right thing, otherwise, just go for array = array * something.\nLet’s see why.  We define two functions that to the eyes of many (including past me) do just the same.\n\nimport numpy as np\n\ndef multiply(array, scalar):\n    array *= scalar  # &lt;-- handy short hand, right?  ;)\n    return array\n\ndef multiply2(array, scalar):\n    array = array * scalar\n    return array\n\nLet’s see them in action\n\na = np.arange(10.)  # dot casts to float to avoid type errors\nb = np.arange(10.)\n\n\na\n\narray([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])\n\n\n\nb\n\narray([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])\n\n\n\nmultiply(a, 2)\n\narray([ 0.,  2.,  4.,  6.,  8., 10., 12., 14., 16., 18.])\n\n\n\nmultiply(a, 2)\n\narray([ 0.,  4.,  8., 12., 16., 20., 24., 28., 32., 36.])\n\n\nHey, wait! What’s going on?\n\na\n\narray([ 0.,  4.,  8., 12., 16., 20., 24., 28., 32., 36.])\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe operation modifies the array in place.\n\n\nLet’s see what the other version of our function does.\n\nmultiply2(b, 2)\n\narray([ 0.,  2.,  4.,  6.,  8., 10., 12., 14., 16., 18.])\n\n\n\nmultiply2(b, 2)\n\narray([ 0.,  2.,  4.,  6.,  8., 10., 12., 14., 16., 18.])\n\n\nThis time the input array stays the same, ie., the modification remained in the scope of the function.\nDespite it being very basic, it is actually more difficult to debug than for the toy example in real life cases.  For instance, in the middle of a long data preprocessing pipeline. If you load your data once and run the preprocessing pipeline once, you will probably not notice the bug (that’s the tricky thing!). But if the loaded data are passed more than once through the pipeline (without reloading the whole data), each pass will be actually feeding different input. For example, if you run K-Fold cross-validation, most likely it won’t crash or anything, but you will be passing K different datasets to your model and your validation will be just rubbish!\nConclusions:  - array *= something is very different from array = array * something  - You’d better be really sure of what you’re doing with array = array * something.\n\nAny bugs, questions, comments, suggestions? Ping me on twitter or drop me an e-mail (fabridamicelli at gmail)."
  },
  {
    "objectID": "posts/2021-10-16-lr_add_constant.html",
    "href": "posts/2021-10-16-lr_add_constant.html",
    "title": "Linear Regression: Don’t forget your bias",
    "section": "",
    "text": "This is a question on Stackoverflow:\n“Why do I get only one parameter from a statsmodels OLS fit?”\nAs of today it has 52K views.\nSo, if you ran into this problem, you’re not alone.\nOLS refers to Ordinary Least Squares, a method to estimate the parameters of a Linear Regression model.\nThe fix:\nInstead of doing just\nimport statsmodels.api as sm\n\nsm.OLS(y, X)\ndo:\n\nX = sm.add_constant(X)    # I am the fix, nice to meet you :)\nsm.OLS(y, X)\nAll good. But why does that fix it?"
  },
  {
    "objectID": "posts/2021-10-16-lr_add_constant.html#the-situation",
    "href": "posts/2021-10-16-lr_add_constant.html#the-situation",
    "title": "Linear Regression: Don’t forget your bias",
    "section": "",
    "text": "This is a question on Stackoverflow:\n“Why do I get only one parameter from a statsmodels OLS fit?”\nAs of today it has 52K views.\nSo, if you ran into this problem, you’re not alone.\nOLS refers to Ordinary Least Squares, a method to estimate the parameters of a Linear Regression model.\nThe fix:\nInstead of doing just\nimport statsmodels.api as sm\n\nsm.OLS(y, X)\ndo:\n\nX = sm.add_constant(X)    # I am the fix, nice to meet you :)\nsm.OLS(y, X)\nAll good. But why does that fix it?"
  },
  {
    "objectID": "posts/2021-10-16-lr_add_constant.html#the-problem",
    "href": "posts/2021-10-16-lr_add_constant.html#the-problem",
    "title": "Linear Regression: Don’t forget your bias",
    "section": "The problem",
    "text": "The problem\nThe basic problem is that, as the documentation states, the default model for which the method fits the parameters is the following:\n\\[\nY = X \\beta + \\mu\n\\]\nwhere \\(Y\\) is the dependent variable, \\(X\\) is the so called “design matrix”, ie your independent variables (predictors) packed into columns, \\(\\beta\\) the coefficients that we are figuring out with ordinary least squares and \\(\\mu\\) the “noise” or “error term”, ie bag of factors that influence our variable \\(Y\\) but we are not including in \\(X\\).\nWhat we actually might need is:\n\\[\nY = X \\beta_1 + \\beta_0 + \\mu\n\\]\n\\(\\beta_0\\) is a constant term, ie the “bias” in our model, also called “intercept”, as we are talking about a line equation and that is the value of y at which a line intecepts such axis when x = 0.\nSo what is the bias?\nHere’s an analogy: Imagine we are trying to predict beer consumption (liters per person per year) based on number of sunny days (per year). The mental model (hypothesis) being that people will spend more time outdoors on sunny days and beer would be their choice. Silly model, but bear with me. We go out and get data for the last 50 years regarding on how much beer people drank and how many sunny days per year there were, so we’d have:\n\\[\nY_{beer} = X \\beta_{sun} + \\beta_0 + \\mu\n\\]\nNow, say we carry out the study in Germany and Indonesia. Then maybe (just maybe) people tend to consume more alcohol in general in one of those countries than in the other, regardless of the number of sunny days. The bias in our model could capture that. In other words, we could see if the “starting point” (the \\(\\beta_0\\) term) is different for the countries, on top of what we could have the influence of our independent variable, by looking at the data separately and fit two different models (one per country):\n\\[\nY_{beer-Indonesia} = X \\beta_{sun} + \\beta_{0-Indonesia} + \\mu\n\\]\n\\[\nY_{beer-Germany} = X \\beta_{sun} + \\beta_{0-Germany} + \\mu\n\\]\nOK. That’s all I will say about linear regression here.\n\nwarning: Don’t take that example too seriously. For the sake of the story I am deliberately using a causal narrative, which is by no means supported by a regression model based on correlations. But you got the point.\n\nAside from that, let’s look at some code examples that will be more illustrative of what the bias is doing in our model."
  },
  {
    "objectID": "posts/2021-10-16-lr_add_constant.html#experiment-with-synthetic-data",
    "href": "posts/2021-10-16-lr_add_constant.html#experiment-with-synthetic-data",
    "title": "Linear Regression: Don’t forget your bias",
    "section": "Experiment with synthetic data",
    "text": "Experiment with synthetic data\nLet’s create a synthetic ground truth dataset so that we can see these models in action on a concrete example.\n\nfrom sklearn.datasets import make_regression\n\nx, y = make_regression(n_samples=200, n_features=1, bias=50, noise=20)\n\nAs the parameter bias shows, the data will have this intrinsic bias. So we know it’s there. Now we will fit a model with and another without bias and observe their behaviour.\n\nfrom sklearn.linear_model import LinearRegression\n\nmodel_with_bias = LinearRegression().fit(x, y)   # default: fit_intercept=True\nmodel_without_bias = LinearRegression(fit_intercept=False).fit(x, y)\n\nLet’s check how well these models fit the training data by plotting predictions coming from both of them:\n\ndef plot_models_fit(x, y, model_with_bias, model_without_bias):\n    plt.figure(figsize=(8, 6))\n    plt.plot(x, y, 'o', label=\"data\", alpha=.5)\n    plt.plot(x, model_with_bias.predict(x),  label=\"model with bias\")\n    plt.plot(x, model_without_bias.predict(x), label=\"model without bias\")\n    plt.ylabel(\"Dependent variable\")\n    plt.xlabel(\"Independent variable\")\n    plt.legend()\n\n\nplot_models_fit(x, y, model_with_bias, model_without_bias)\n\n\n\n\nAs we see, the two models produce different outcomes. The one without bias “forces” the line to pass through the origin. The slope may be similar because of ordinary least squares minimizes the vertical (square) distance from the line to the data points, but that really depends on the data at hand, as we can see in the following example with different ground truth data:\n\nx, y = make_regression(n_samples=100, n_features=1, bias=300, noise=80)\n\nmodel_with_bias = LinearRegression().fit(x, y)   # default: fit_intercept=True\nmodel_without_bias = LinearRegression(fit_intercept=False).fit(x, y)\n\nplot_models_fit(x, y, model_with_bias, model_without_bias)\n\n\n\n\nFinally, what if the data indeed does not have such a bias? Let’s try it:\n\nx, y = make_regression(n_samples=100, n_features=1, bias=0, noise=80)\n\nmodel_with_bias = LinearRegression().fit(x, y)   # default: fit_intercept=True\nmodel_without_bias = LinearRegression(fit_intercept=False).fit(x, y)\n\nplot_models_fit(x, y, model_with_bias, model_without_bias)\n\n\n\n\nIn general, the models indeed tend to become more similar (provided enough data points are given). But, again, the exact outcome depends on the data at hand."
  },
  {
    "objectID": "posts/2021-10-16-lr_add_constant.html#take-home-messages",
    "href": "posts/2021-10-16-lr_add_constant.html#take-home-messages",
    "title": "Linear Regression: Don’t forget your bias",
    "section": "Take-home messages",
    "text": "Take-home messages\n\nExcluding the bias constrains the parameters fit such that we will only get a similarly good (or bad) model compared to the one with bias if the data happens to not have such a bias.\nscikit-learn’sLinearRegression model fits the intercept per default. statsmodels OLS does not.\nWhenever in doubt in a situation like this, just try it out. Fake some data that you understand and run your own experiments.\n\nFin\n\nAny bugs, questions, comments, suggestions? Ping me on twitter or drop me an e-mail (fabridamicelli at gmail)."
  },
  {
    "objectID": "posts/2020-05-20-achtung-german-csv-reader.html",
    "href": "posts/2020-05-20-achtung-german-csv-reader.html",
    "title": "Achtung: Watch out, German csv readers!",
    "section": "",
    "text": "TL;DR: pandas.read_csv considers the word “null” as a NaN, which also means “zero” in German. The arguments na_values and keep_default_na offer a solution.\nIt’s Friday an you set out to build a very sophisticated numbers translator in several languages:\n\nimport pandas as pd\n\nnumbers = pd.DataFrame({\n   \"Spanish\": [\"cero\", \"uno\", \"dos\", \"tres\"],\n   \"English\": [\"zero\", \"one\", \"two\", \"three\"],\n   \"German\": [\"null\", \"eins\", \"zwei\", \"drei\"],\n})\nnumbers.index = numbers.index.rename(\"Number\")\nnumbers\n\n\n\n\n\n\n\n\nSpanish\nEnglish\nGerman\n\n\nNumber\n\n\n\n\n\n\n\n0\ncero\nzero\nnull\n\n\n1\nuno\none\neins\n\n\n2\ndos\ntwo\nzwei\n\n\n3\ntres\nthree\ndrei\n\n\n\n\n\n\n\nIf you want to know how to say 3 in Spanish, you do:\n\nnumbers.loc[3, \"Spanish\"]\n\n'tres'\n\n\nNice. You save the super advanced translator for later use and go off for a relaxed weekend.\n\nnumbers.to_csv(\"numbers\")\n\nBack to work on Monday, your German friend drops by your office and you want to proudly show what you’ve created.  So you load your “translator” and go like:  – Ask me how to say any number!  – OK. Let’s start easy: how do you say zero in German?\n“That I can do”, you think and type:\n\n# Load the awesome translator\nnumbers_loaded = pd.read_csv(\"numbers\", index_col=\"Number\")\n# And get the translation\nnumbers_loaded.loc[0, \"German\"]\n\nnan\n\n\nOh no, that’s no good!  You get out of the embarrassing situation saying it is actually a beta version, and, and, and. The harm is done and your friend leaves the office skeptical – to say the least.\nWhat’s was the problem?  The answer is in the docstrings of the function pd.read_csv. If we look carefully at which values pandas considers as NaN per default we find the following:\n\"\"\"\nna_values : scalar, str, list-like, or dict, optional\n    Additional strings to recognize as NA/NaN. If dict passed, specific\n    per-column NA values.  By default the following values are interpreted as\n    NaN: '', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan',\n    '1.#IND', '1.#QNAN', 'N/A', 'NA', 'NULL', 'NaN', 'n/a', 'nan',\n    'null'.\n\"\"\"\nThere it is: “null” is in the list!\nSolution:  We need to do two things: - Pass other values without “null” (and “NULL” if you’re not sure everything is lowercase). - Tell pandas not to keep the defaults (otherwise it will use both the defaults and the passed values).\n\nna_values = [\n    '', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', \n    '-nan', '1.#IND', '1.#QNAN', 'N/A', 'NA', 'NaN', 'n/a', 'nan'\n]\n\nnumbers_loaded = pd.read_csv(\n    \"numbers\", \n     index_col=\"Number\",\n     na_values=na_values, \n     keep_default_na=False\n)\n                            \nnumbers_loaded\n\n\n\n\n\n\n\n\nSpanish\nEnglish\nGerman\n\n\nNumber\n\n\n\n\n\n\n\n0\ncero\nzero\nnull\n\n\n1\nuno\none\neins\n\n\n2\ndos\ntwo\nzwei\n\n\n3\ntres\nthree\ndrei\n\n\n\n\n\n\n\nNow we can ask:\n\nnumbers_loaded.loc[0, \"German\"]\n\n'null'\n\n\nThat will keep your German friends happy :)\n\nAny bugs, questions, comments, suggestions? Ping me on twitter or drop me an e-mail (fabridamicelli at gmail)."
  },
  {
    "objectID": "posts/2022-01-29-chainmap.html",
    "href": "posts/2022-01-29-chainmap.html",
    "title": "from collections import ChainMap",
    "section": "",
    "text": "The built-in collections module is a handy bag of tools to be aware of. Here we explore collections.ChainMap, an elegant solution to efficiently carry out a look up over more than one dictionary.\n\n\n# Here's the gist of it, watch the video for more details.\n\ndict1 = {\n    \"a\": 1, \n    \"b\": 2,\n    \"c\": 3,\n}    \n\ndict2 = {\n    \"d\": 4,\n    \"f\": 0,\n}    \n\ndict3 = {\n    \"g\": 6,\n    \"h\": 7,\n    \"f\": 10,\n}    \n\ndicts = (dict1, dict3, dict2)\n\nkey = \"f\"\n\n\nfor d in dicts:\n    val = d.get(key)\n    if val is not None:\n        print(val)\n        break\n\n10\n\n\n\nfrom collections import ChainMap\n\nChainMap(*dicts)[key]\n\n10\n\n\n\nAny bugs, questions, comments, suggestions? Ping me on twitter or drop me an e-mail (fabridamicelli at gmail)."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Fabrizio Damicelli",
    "section": "",
    "text": "Hi there! 👋 I’m Fabrizio. Thanks for your visit!\nCurrently a Data Scientist based in Hamburg (Germany), working on ~real-time fraud detection with neural networks and large datasets at one of the largest e-commerce platforms in Germany.\nI finished my PhD in 2021 at the Institute of Computational Neuroscience in Hamburg - see research for details. During that time I also created a couple of open source Python packages that I still maintain.\nI enjoy 📝 technical writing and once in while make a 🎥 video covering Python/ML/Data Science topics.\nI like music a lot, Python, reading, simple ideas behind complex phenomena and data-driven solutions. I prefer strong intuitions and simple words to unnecessary sophistication and fancy jargon.\nGet in touch on twitter, LinkedIn or email (fabridamicelli at gmail)."
  }
]